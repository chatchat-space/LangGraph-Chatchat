import rich
import uuid
import asyncio

import streamlit as st
from langgraph.graph.state import CompiledStateGraph
from streamlit_extras.bottom_container import bottom

from chatchat.webui_pages.utils import *
from chatchat.webui_pages.dialogue.dialogue import list_tools
from chatchat.server.agent.graphs_factory.graphs_registry import (
    serialize_content,
    list_graph_titles_by_label,
    get_graph_class_by_label_and_title
)
from chatchat.server.utils import (
    build_logger,
    get_config_models,
    get_config_platforms,
    get_default_llm,
    get_tool,
    create_agent_models,
)

logger = build_logger()


def init_conversation_id():
    if "conversation_id" not in st.session_state:
        st.session_state["conversation_id"] = str(uuid.uuid4())


def extract_node_and_response(data):
    # è·å–ç¬¬ä¸€ä¸ªé”®å€¼å¯¹ï¼Œä½œä¸º node
    if not data:
        raise ValueError("æ•°æ®ä¸ºç©º")

    # è·å–ç¬¬ä¸€ä¸ªé”®åŠå…¶å¯¹åº”çš„å€¼
    node = next(iter(data))
    response = data[node]

    return node, response


async def handle_user_input(graph: CompiledStateGraph,
                            graph_input: Any,
                            graph_config: Dict,
                            graph_class: Any):
    events = graph.astream(input=graph_input, config=graph_config, stream_mode="updates")
    if events:
        # Display assistant response in chat message container
        with st.chat_message("assistant"):
            response_last = ""
            async for event in events:
                node, response = extract_node_and_response(event)
                # debug
                print(f"--- node: {node} ---")
                rich.print(response)
                # è·å– event
                response = await graph_class.handle_event(node=node, event=response)
                # å°† event è½¬åŒ–ä¸º json
                response = serialize_content(response)
                # print("after serialize_content response:")
                # rich.print(response)
                response_last = response["content"]

                if node == "history_manager":  # history_manager node ä¸ºå†…éƒ¨å®ç°, ä¸å¤–æ˜¾
                    continue
                with st.status(node, expanded=True) as status:
                    st.json(response, expanded=True)
                    status.update(
                        label=node, state="complete", expanded=False
                    )
                # Add assistant response to chat history
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "node": node,
                    "expanded": False,
                    "type": "json"  # æ ‡è¯†ä¸ºJSONç±»å‹
                })
            st.markdown(response_last)


@st.dialog("è¾“å…¥åˆå§‹åŒ–å†…å®¹", width="large")
def article_generation_init_setting():
    article_links = st.text_area("æ–‡ç« é“¾æ¥")
    image_links = st.text_area("å›¾ç‰‡é“¾æ¥")

    if st.button("ç¡®è®¤"):
        st.session_state["article_links"] = article_links
        st.session_state["image_links"] = image_links
        # å°† article_generation_init_break_point çŠ¶æ€æ‰­è½¬ä¸º True, åç»­å°†è¿›è¡Œ update_state åŠ¨ä½œ
        st.session_state["article_generation_init_break_point"] = True

        user_input = (f"æ–‡ç« é“¾æ¥: {article_links}\n"
                      f"å›¾ç‰‡é“¾æ¥: {image_links}")
        with st.chat_message("user"):
            st.markdown(user_input)
        st.session_state.messages.append({
            "role": "user",
            "content": user_input,
            "type": "text"  # æ ‡è¯†ä¸ºæ–‡æœ¬ç±»å‹
        })

        st.rerun()


@st.dialog("æ¨¡å‹é…ç½®", width="large")
def llm_model_setting():
    cols = st.columns(3)
    platforms = ["æ‰€æœ‰"] + list(get_config_platforms())
    platform = cols[0].selectbox("æ¨¡å‹å¹³å°è®¾ç½®(Platform)", platforms)
    llm_models = list(
        get_config_models(
            model_type="llm", platform_name=None if platform == "æ‰€æœ‰" else platform
        )
    )
    llm_model = cols[1].selectbox("æ¨¡å‹è®¾ç½®(LLM)", llm_models)
    temperature = cols[2].slider("æ¸©åº¦è®¾ç½®(Temperature)", 0.0, 1.0, value=st.session_state["temperature"])

    if st.button("ç¡®è®¤"):
        st.session_state["platform"] = platform
        st.session_state["llm_model"] = llm_model
        st.session_state["temperature"] = temperature
        st.rerun()


def graph_rag_page(api: ApiRequest, is_lite: bool = False):
    # åˆå§‹åŒ–ä¼šè¯ id
    init_conversation_id()

    # åˆ›å»º streamlit æ¶ˆæ¯ç¼“å­˜
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # åˆå§‹åŒ–æ¨¡å‹é…ç½®
    if "platform" not in st.session_state:
        st.session_state["platform"] = "æ‰€æœ‰"
    if "llm_model" not in st.session_state:
        st.session_state["llm_model"] = get_default_llm()
        logger.info("default llm model: {}".format(st.session_state["llm_model"]))
    if "temperature" not in st.session_state:
        st.session_state["temperature"] = 0.01
    if "prompt" not in st.session_state:
        st.session_state["prompt"] = ""
    if "selected_kb" not in st.session_state:
        st.session_state["selected_kb"] = Settings.kb_settings.DEFAULT_KNOWLEDGE_BASE
    if "history_len" not in st.session_state:
        st.session_state["history_len"] = Settings.model_settings.HISTORY_LEN
    if "kb_top_k" not in st.session_state:
        st.session_state["kb_top_k"] = Settings.kb_settings.VECTOR_SEARCH_TOP_K
    if "score_threshold" not in st.session_state:
        st.session_state["score_threshold"] = Settings.kb_settings.SCORE_THRESHOLD

    with st.sidebar:
        tabs_1 = st.tabs(["å·¥ä½œæµè®¾ç½®"])
        with tabs_1[0]:
            placeholder = st.empty()

            def on_kb_change():
                st.toast(f"å·²åŠ è½½çŸ¥è¯†åº“ï¼š {st.session_state.selected_kb}")

            with placeholder.container():
                rag_graph_names = list_graph_titles_by_label(label="rag")
                selected_graph = st.selectbox(
                    "é€‰æ‹©çŸ¥è¯†åº“é—®ç­”å·¥ä½œæµ",
                    rag_graph_names,
                    format_func=lambda x: x,
                    key="selected_graph",
                    help="å¿…é€‰ï¼Œä¸åŒçš„å·¥ä½œæµçš„åç«¯ agent çš„é€»è¾‘ä¸åŒï¼Œä»…æ”¯æŒå•é€‰"
                )

                kb_list = [x["kb_name"] for x in api.list_knowledge_bases()]
                selected_kb = st.selectbox(
                    "è¯·é€‰æ‹©çŸ¥è¯†åº“ï¼š",
                    kb_list,
                    on_change=on_kb_change,
                    key="selected_kb",
                )

                tools_list = list_tools(api)
                # tool_names = ["None"] + list(tools_list)
                # selected_tools demo: ['search_internet', 'search_youtube']
                selected_tools = st.multiselect(
                    label="é€‰æ‹©å·¥å…·",
                    options=list(tools_list),
                    format_func=lambda x: tools_list[x]["title"],
                    key="selected_tools",
                    default="search_local_knowledgebase",
                    help="æ”¯æŒå¤šé€‰"
                )
                selected_tool_configs = {
                    name: tool["config"]
                    for name, tool in tools_list.items()
                    if name in selected_tools
                }

        tabs_2 = st.tabs(["é—®ç­”è®¾ç½®"])
        with tabs_2[0]:
            history_len = st.number_input("å†å²å¯¹è¯è½®æ•°ï¼š", 0, 20, key="history_len")
            kb_top_k = st.number_input("åŒ¹é…çŸ¥è¯†æ¡æ•°ï¼š", 1, 20, key="kb_top_k")
            # Bge æ¨¡å‹ä¼šè¶…è¿‡ 1
            score_threshold = st.slider("çŸ¥è¯†åŒ¹é…åˆ†æ•°é˜ˆå€¼ï¼š", 0.0, 2.0, step=0.01, key="score_threshold", help="åˆ†æ•°è¶Šå°åŒ¹é…åº¦è¶Šå¤§")

        st.tabs(["å·¥ä½œæµæµç¨‹å›¾"])

    selected_tools_configs = list(selected_tool_configs)

    st.title("çŸ¥è¯†åº“èŠå¤©")
    with st.chat_message("assistant"):
        st.write("Hello ğŸ‘‹ğŸ˜Šï¼Œæˆ‘æ˜¯æ™ºèƒ½çŸ¥è¯†åº“é—®ç­”æœºå™¨äººï¼Œè¯•ç€è¾“å…¥ä»»ä½•å†…å®¹å’Œæˆ‘èŠå¤©å‘¦ï½ï¼ˆps: å¯å°è¯•åˆ‡æ¢ä¸åŒçŸ¥è¯†åº“ï¼‰")

    with bottom():
        cols = st.columns([1, 0.2, 15, 1])
        if cols[0].button(":gear:", help="æ¨¡å‹é…ç½®"):
            llm_model_setting()
        if cols[-1].button(":wastebasket:", help="æ¸…ç©ºå¯¹è¯"):
            st.session_state["messages"] = []
            st.rerun()
        user_input = cols[2].chat_input("å¦‚ä½•ç»™ tcr å®ä¾‹å¼€å¯å…¬ç½‘è®¿é—®ï¼Ÿ(æ¢è¡Œ:Shift+Enter)")

    # get_tool() æ˜¯æ‰€æœ‰å·¥å…·çš„åç§°å’Œå¯¹è±¡çš„ dict çš„åˆ—è¡¨
    all_tools = get_tool().values()
    tools = [tool for tool in all_tools if tool.name in selected_tools_configs]

    # åˆ›å»º llm å®ä¾‹
    # todo: max_tokens è¿™é‡Œæœ‰é—®é¢˜, None åº”è¯¥æ˜¯ä¸é™åˆ¶, ä½†æ˜¯ç›®å‰ llm ç»“æœä¸º 4096
    llm_model = st.session_state["llm_model"]
    llm = create_agent_models(configs=None,
                              model=llm_model,
                              max_tokens=None,
                              temperature=st.session_state["temperature"],
                              stream=True)
    rich.print(llm)

    # åˆ›å»º langgraph å®ä¾‹
    graph_class = get_graph_class_by_label_and_title(label="rag", title=selected_graph)
    graph_class = graph_class(llm=llm,
                              tools=tools,
                              history_len=history_len,
                              knowledge_base=selected_kb,
                              top_k=kb_top_k,
                              score_threshold=score_threshold)
    graph = graph_class.get_graph()
    if not graph:
        raise ValueError(f"Graph '{selected_graph}' is not registered.")

    # langgraph é…ç½®æ–‡ä»¶
    graph_config = {
        "configurable": {
            "thread_id": st.session_state["conversation_id"]
        },
    }

    logger.info(f"graph: '{selected_graph}', configurable: '{graph_config}'")

    # ç»˜åˆ¶æµç¨‹å›¾å¹¶ç¼“å­˜
    graph_flow_image_name = f"{selected_graph}_flow_image"
    if graph_flow_image_name not in st.session_state:
        graph_png_image = graph.get_graph().draw_mermaid_png()
        st.session_state[graph_flow_image_name] = graph_png_image
    st.sidebar.image(st.session_state[graph_flow_image_name], use_column_width=True)

    # å‰ç«¯å­˜å‚¨å†å²æ¶ˆæ¯(ä»…ä½œä¸º st.rerun() æ—¶çš„ UI å±•ç¤º)
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["type"] == "json":
                with st.status(message["node"], expanded=message["expanded"]) as status:
                    st.json(message["content"], expanded=message["expanded"])
                    status.update(
                        label=message["node"], state="complete", expanded=False
                    )
            elif message["type"] == "text":
                st.markdown(message["content"])

    # å¯¹è¯ä¸»æµç¨‹
    if user_input:
        with st.chat_message("user"):
            st.markdown(user_input)
        st.session_state.messages.append({
            "role": "user",
            "content": user_input,
            "type": "text"
        })

        # Run the async function in a synchronous context
        graph_input = {"messages": [("user", user_input)]}
        asyncio.run(handle_user_input(graph=graph,
                                      graph_input=graph_input,
                                      graph_config=graph_config,
                                      graph_class=graph_class))
